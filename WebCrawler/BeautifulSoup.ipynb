{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7172234d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## parser.py\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c07eea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTTP GET Request\n",
    "req = requests.get('https://beomi.github.io/beomi.github.io_old/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ceb7313",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HTML 소스 가져오기\n",
    "html = req.text\n",
    "\n",
    "#HTTP Header 가져오기\n",
    "#header = req.headers\n",
    "\n",
    "#HTTP Status 가져오기(200: 정상)\n",
    "#status = req.status_code\n",
    "\n",
    "#HTTP가 정상적으로 되었는지 T/F\n",
    "#is_ok = req.ok\n",
    "\n",
    "#BeautifulSoup으로 html 소스를 python객체로 변환하기\n",
    "# 첫 인자는 html소스코드, 두번째 인자는 어떤 parser를 이용할지 명시\n",
    "# 이 파일에서는 Python 내장 html.parser를 이용\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "294e0c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title: 나만의 웹 크롤러 만들기(4): Django로 크롤링한 데이터 저장하기\n",
      "href: /beomi.github.io_old/python/2017/02/28/HowToMakeWebCrawler-Save-with-Django.html\n",
      "title: 나만의 웹 크롤러 만들기(3): Selenium으로 무적 크롤러 만들기\n",
      "href: /beomi.github.io_old/python/2017/02/27/HowToMakeWebCrawler-With-Selenium.html\n",
      "title: Django에 Social Login 붙이기: Django세팅부터 Facebook/Google 개발 설정까지\n",
      "href: /beomi.github.io_old/python/2017/02/08/Setup-SocialAuth-for-Django.html\n",
      "title: Django에 Custom인증 붙이기\n",
      "href: /beomi.github.io_old/python/2017/02/01/Django-CustomAuth.html\n",
      "title: 나만의 웹 크롤러 만들기(2): Login with Session\n",
      "href: /beomi.github.io_old/python/2017/01/20/HowToMakeWebCrawler-With-Login.html\n",
      "title: 나만의 웹 크롤러 만들기 with Requests/BeautifulSoup\n",
      "href: /beomi.github.io_old/python/2017/01/19/HowToMakeWebCrawler.html\n",
      "title: Celery로 TelegramBot 알림 보내기\n",
      "href: /beomi.github.io_old/2016/12/27/TelegramBot-with-Celery.html\n",
      "title: Virtualenv/VirtualenvWrapper OS별 설치&이용법\n",
      "href: /beomi.github.io_old/2016/12/27/HowToSetup-Virtualenv-VirtualenvWrapper.html\n",
      "title: [DjangoTDDStudy] #02: UnitTest 이용해 기능 테스트 하기\n",
      "href: /beomi.github.io_old/djangotddstudy/2016/12/26/Django-TDD-Study-02-Using-UnitTest.html\n",
      "title: [DjangoTDDStudy] #01: 개발환경 세팅하기(Selenium / ChromeDriver)\n",
      "href: /beomi.github.io_old/djangotddstudy/2016/12/26/Django-TDD-Study-01-Setting-DevEnviron.html\n",
      "title: [DjangoTDDStudy] #00: 스터디를 시작하며\n",
      "href: /beomi.github.io_old/djangotddstudy/2016/12/26/Django-TDD-Study-00-Starting-Study.html\n",
      "title: Fabric Put 커맨드가 No Such File Exception을 반환할 때 해결법\n",
      "href: /beomi.github.io_old/2016/12/21/Fabric-Put-Command-No-Such-File-Exception.html\n",
      "title: CKEditor의 라이센스와 오픈소스 라이센스\n",
      "href: /beomi.github.io_old/2016/12/21/CKEditor-Lisence-and-Pricing.html\n",
      "title: ReactNative The Basis 번역을 끝냈습니다.\n",
      "href: /beomi.github.io_old/translation/2016/12/20/ReactNative-Translation-Intro-Finish.html\n",
      "title: [React Native 번역]#01: 시작하기\n",
      "href: /beomi.github.io_old/translation/2016/11/15/ReactNative-Translation-01-getting-started.html\n",
      "title: [번역] 장고(Django)와 함께하는 Celery 첫걸음\n",
      "href: /beomi.github.io_old/django-celery/programming/python/translation/2016/11/04/eb-b2-88-ec-97-ad-ec-9e-a5-ea-b3-a0django-ec-99-80-ed-95-a8-ea-bb-98-ed-95-98-eb-8a-94-celery-ec-b2-ab-ea-b1-b8-ec-9d-8c.html\n",
      "title: Chrome Native Adblockr 대체하기\n",
      "href: /beomi.github.io_old/tech/2016/09/14/chrome-native-adblockr-eb-8c-80-ec-b2-b4-ed-95-98-ea-b8-b0.html\n",
      "title: CustoMac 설치 분투기\n",
      "href: /beomi.github.io_old/dev%20env%20setup/mac%20/%20os%20x/tech/2016/08/09/customac-ec-84-a4-ec-b9-98-eb-b6-84-ed-88-ac-ea-b8-b0.html\n",
      "title: Ubuntu14.04에 OhMyZsh 설치\n",
      "href: /beomi.github.io_old/dev%20env%20setup/tech/ubuntu%20/%20debian/2016/07/22/ubuntu14-04-ec-97-90-ohmyzsh-ec-84-a4-ec-b9-98.html\n",
      "title: Ubuntu14.04에서 pip로 mysqlclient 설치 실패시\n",
      "href: /beomi.github.io_old/programming/python/tech/2016/07/22/ubuntu14-04-ec-97-90-ec-84-9c-pip-eb-a1-9c-mysqlclient-ec-84-a4-ec-b9-98-ec-8b-a4-ed-8c-a8-ec-8b-9c.html\n",
      "title: Ubuntu14.04에서 Python3기반 virtualenvwrapper 설치\n",
      "href: /beomi.github.io_old/mac%20/%20os%20x/programming/python/tech/2016/07/22/ubuntu14-04-ec-97-90-ec-84-9c-python3-ea-b8-b0-eb-b0-98-virtualenvwrapper-ec-84-a4-ec-b9-98.html\n",
      "title: mac OS X에서 pip virtualenvwrapper 설치 시 uninstalling six 에서 Exception 발생 시\n",
      "href: /beomi.github.io_old/mac%20/%20os%20x/programming/python/tech/2016/07/21/mac-os-x-ec-97-90-ec-84-9c-pip-virtualenvwrapper-ec-84-a4-ec-b9-98-ec-8b-9c-uninstalling-six-ec-97-90-ec-84-9c-exception-eb-b0-9c-ec-83-9d-ec-8b-9c.html\n",
      "title: Fabric for Python3 (Fabric3)\n",
      "href: /beomi.github.io_old/programming/python/2016/07/17/fabric-for-python3-fabric3.html\n",
      "title: Windows에서 pip로 mysqlclient 설치 실패시(python3.4/3.5)\n",
      "href: /beomi.github.io_old/programming/python/2016/06/04/windows-ec-97-90-ec-84-9c-pip-eb-a1-9c-mysqlclient-ec-84-a4-ec-b9-98-ec-8b-a4-ed-8c-a8-ec-8b-9cpython3-43-5.html\n",
      "title: 맥에서 윈도RDP로 접속시 한영전환하기.\n",
      "href: /beomi.github.io_old/mac%20/%20os%20x/tech/2016/05/27/eb-a7-a5-ec-97-90-ec-84-9c-ec-9c-88-eb-8f-84rdp-eb-a1-9c-ec-a0-91-ec-86-8d-ec-8b-9c-ed-95-9c-ec-98-81-ec-a0-84-ed-99-98-ed-95-98-ea-b8-b0.html\n",
      "title: pip로 mysqlclient설치 중 mac os x에서 egg_info / OSError 발생시 대처방법\n",
      "href: /beomi.github.io_old/programming/python/2016/05/27/pip-eb-a1-9c-mysqlclient-ec-84-a4-ec-b9-98-ec-a4-91-mac-os-x-ec-97-90-ec-84-9c-egg_info-oserror-eb-b0-9c-ec-83-9d-ec-8b-9c-eb-8c-80-ec-b2-98-eb-b0-a9-eb-b2-95.html\n"
     ]
    }
   ],
   "source": [
    "my_titles = soup.select(\n",
    "    'h3 > a'\n",
    ")\n",
    "\n",
    "# my_titles 는 list객체\n",
    "for title in my_titles:\n",
    "    #Tag 안의 텍스트\n",
    "    print('title: '+ title.text)\n",
    "    # Tag의 속성을 가져오기(ex: href속성)\n",
    "    print ('href: ' + title.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90ad5ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "#예제 정리\n",
    "# Python 파일과 같은 위치에 result.json을 만들어 저장하는 예제다.\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import os\n",
    "\n",
    "#python file 위치 설정\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(''))\n",
    "req = requests.get('https://beomi.github.io/beomi.github.io_old/')\n",
    "html = req.text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "my_titles = soup.select(\n",
    "    'h3 > a'\n",
    ")\n",
    "\n",
    "data = {}\n",
    "\n",
    "for title in my_titles:\n",
    "    data[title.text] = title.get('href')\n",
    "    \n",
    "with open(os.path.join(BASE_DIR, 'result.json'), 'w+') as json_file:\n",
    "    json.dump(data, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc8d52b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 웹은 대다수가 HTTP기반으로 동작, 하지만 HTTP가 구현된 방식에서 웹 서버와 클라이언트는 지속적으로 연결을 유지한 상태가 아니라 요청-응답의 반복일 뿐이기 때문에, 이전 요청과 새로운 요청이 같은 사용자(같은 브라우저)에서 이루어졌는지를 확인하는 방법이 필요합니다.\n",
    "\n",
    "# 1. 쿠키: 유저가 웹 사이트를 방문할 때 사용자의 브라우저에 심겨지는 작은 파일. Key-Value형식으로 로컬 브라우저에 저장\n",
    "### 이 쿠키의 정보를 읽어 HTTP요청에 대해 브라우저를 식별한다. 하지만, 로컬에 저장된다는 근원적인 문제로 인해 악의적 사용자가 쿠키를 변조하거나 탈취해 정상적이지 않은 쿠키로 서버에 요청을 보낼 수 있습니다.\n",
    "### 만약 로그인 식별을 로컬쿠키만을 신뢰해 로그인을 한 상태로 서버가 인식하면 쿠키변조를 통해 마치 관리자나 다른 유저처럼 행동할 수도 있다.\n",
    "\n",
    "# 2. 세션: 브라우저가 웹 서버에 요청을 한 경우 서버 내에 해당 세션 정보를 파일이나 DB에 저장하고 클라이언트의 브라우저에 session-id라는 임의의 긴 문자열을 준다.\n",
    "# 이때 사용되는 쿠키는 클라이언트와 서버간 연결이 끊어진 경우 삭제되는 메모리 쿠키 이용\n",
    "\n",
    "#세션을 이용한 코딩\n",
    "\n",
    "import requests\n",
    "\n",
    "s= requests.Session()\n",
    "\n",
    "req = s.get('https://www.clien.net/service/')\n",
    "\n",
    "html = req.text\n",
    "header = req.headers\n",
    "status = req.status_code\n",
    "is_ok = req.ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9c1c813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 웹은 대다수가 HTTP기반으로 동작, 하지만 HTTP가 구현된 방식에서 웹 서버와 클라이언트는 지속적으로 연결을 유지한 상태가 아니라 요청-응답의 반복일 뿐이기 때문에, 이전 요청과 새로운 요청이 같은 사용자(같은 브라우저)에서 이루어졌는지를 확인하는 방법이 필요합니다.\n",
    "\n",
    "# 1. 쿠키: 유저가 웹 사이트를 방문할 때 사용자의 브라우저에 심겨지는 작은 파일. Key-Value형식으로 로컬 브라우저에 저장\n",
    "### 이 쿠키의 정보를 읽어 HTTP요청에 대해 브라우저를 식별한다. 하지만, 로컬에 저장된다는 근원적인 문제로 인해 악의적 사용자가 쿠키를 변조하거나 탈취해 정상적이지 않은 쿠키로 서버에 요청을 보낼 수 있습니다.\n",
    "### 만약 로그인 식별을 로컬쿠키만을 신뢰해 로그인을 한 상태로 서버가 인식하면 쿠키변조를 통해 마치 관리자나 다른 유저처럼 행동할 수도 있다.\n",
    "\n",
    "# 2. 세션: 브라우저가 웹 서버에 요청을 한 경우 서버 내에 해당 세션 정보를 파일이나 DB에 저장하고 클라이언트의 브라우저에 session-id라는 임의의 긴 문자열을 준다.\n",
    "# 이때 사용되는 쿠키는 클라이언트와 서버간 연결이 끊어진 경우 삭제되는 메모리 쿠키 이용\n",
    "\n",
    "#세션을 이용한 코딩\n",
    "\n",
    "import requests\n",
    "\n",
    "s= requests.Session()\n",
    "\n",
    "req = s.get('https://www.clien.net/service/')\n",
    "\n",
    "html = req.text\n",
    "header = req.headers\n",
    "status = req.status_code\n",
    "is_ok = req.ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1650e30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Session 생성, with 구문 안에서 유지\n",
    "with requests.Session() as s:\n",
    "    #HTTP GET request: requests 대신 s 객체를 사용한다.\n",
    "    req = s.get('https://www.clien.net/service/')\n",
    "\n",
    "    html = req.text\n",
    "    header = req.headers\n",
    "    status = req.status_code\n",
    "    is_ok = req.ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe2f8882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404\n"
     ]
    }
   ],
   "source": [
    "# 클리앙 페이지의 로그인 쪽을 inspect하면 input 필드들의 name을 확인할 수 있다.\n",
    "# _csrf : CSRF는 사용자의 요청이 악의적이거나 제 3자에 의해 변조된 요청이 아닌지 확인해주는 보안도구 중 하나\n",
    "# CSRF를 사용하는 경우 CSRF값이 없는 폼 전송은 위험한 요청으로 생각하고 폼을 받아들이지 않습니다.\n",
    "# auth.login() 함수 입력 유무 확인함수\n",
    "\n",
    "# parser.py\n",
    "import requests\n",
    "\n",
    "# 로그인할 유저정보를 넣어주자 (모두 문자열)\n",
    "LOGIN_INFO = {\n",
    "    'userId': '사용자이름',\n",
    "    'userPassword': '사용자패스워드'\n",
    "}\n",
    "\n",
    "# Session 생성, with 구문 안에서 유지\n",
    "with requests.Session() as s:\n",
    "    # HTTP POST request: 로그인을 위해 POST url와 함께 전송될 data를 넣어주자.\n",
    "    login_req = s.post('https://www.clien.net/service/login', data=LOGIN_INFO)\n",
    "    # 어떤 결과가 나올까요?\n",
    "    print(login_req.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d4e5866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2b7ba157-aaf4-4a45-a85c-15f179ddd6b9\n",
      "{'userId': '사용자이름', 'userPassword': '사용자패스워드', '_csrf': '2b7ba157-aaf4-4a45-a85c-15f179ddd6b9'}\n",
      "200\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# _csrf값 가져오기\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "# 로그인할 유저 정보를 넣어줍시다. (모두 문자열)\n",
    "LOGGIN_INFO = {\n",
    "    'userId' : 'myidid',\n",
    "    'userPassword' : 'mypassword123'\n",
    "}\n",
    "\n",
    "# Session 생성, with 구문 안에서 유지\n",
    "with requests.Session() as s:\n",
    "    # 우선 클리앙 홈페이지에 입장\n",
    "    first_page = s.get('https://www.clien.net/service')\n",
    "    html = first_page.text\n",
    "    soup = bs(html, 'html.parser')\n",
    "    csrf = soup.find('input',{'name': '_csrf'}) \n",
    "    #input 태그 중에서 name이 _csrf인 것을 찾습니다.\n",
    "    print(csrf['value']) # 위에서 찾은 태그의 value를 가져옵니다.\n",
    "    \n",
    "    #이제 LOGIN_INFO에 csrf값을 넣어줍시다.\n",
    "    # (p.s.) Python3에서 두 dict를 합치는 방법은 {**dict1, **dict2}으로 dict들을 unpacking하는 것입니다.\n",
    "    \n",
    "    LOGIN_INFO = {**LOGIN_INFO, **{'_csrf': csrf['value']}}\n",
    "    print(LOGIN_INFO)\n",
    "    \n",
    "    #이제 다시 로그인을 해봅시다.\n",
    "    login_req = s.post('https://www.clien.net/service/login', data=LOGIN_INFO)\n",
    "    print(login_req.status_code)\n",
    "    #로그인이 되지 않으면 경고를 띄워준다.\n",
    "    \n",
    "    if login_req.status_code != 200:\n",
    "        raise Exception('로그인 안됨')\n",
    "        \n",
    "    #여기부터는 로그인 된 세션\n",
    "    post_one = s.get('https://www.clien.net/service/board/rule/10707408')\n",
    "    soup = bs(post_one.text, 'html.parser') # Soup으로 만들어 주자.\n",
    "    #아래 css selector는 공지글 제목을 콕하고 집어줍니다.\n",
    "    title = soup.select('#div_content > div.post-title > div.title-subject > div')\n",
    "    contents = soup.select('#div_content > div.post.box > div.post-content > div.post-article.fr-view')\n",
    "    # HTML을 제대로 파싱한 뒤에는 .text속성을 이용합니다.\n",
    "    \n",
    "    #print(title[0].text) # 글제목의 문자만을 가져와 본다.\n",
    "    #[0]을 하는 이유는 select로 하나만 가져와도 title 자체는 리스트이기 때문\n",
    "    #print((contents[0].text))\n",
    "    print(len(title))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d23b9349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9f884364-77fa-4622-8731-c57e4a829bfd\n",
      "{'userId': 'myidid', 'userPassword': 'mypassword123', '_csrf': '9f884364-77fa-4622-8731-c57e4a829bfd'}\n",
      "200\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-9a53c97e518b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mcontents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'#div_content > div.post.box > div.post-content > div.post-article.fr-view'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;31m# HTML을 제대로 파싱한 뒤에는 .text속성을 이용합니다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 글제목의 문자만을 가져와봅시다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m     \u001b[1;31m# [0]을 하는 이유는 select로 하나만 가져와도 title자체는 리스트이기 때문입니다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;31m# 즉, 제목 글자는 title이라는 리스트의 0번(첫번째)에 들어가 있습니다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# parser.py\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "# 로그인할 유저정보를 넣어줍시다. (모두 문자열입니다!)\n",
    "LOGIN_INFO = {\n",
    "    'userId': 'myidid',\n",
    "    'userPassword': 'mypassword123'\n",
    "}\n",
    "\n",
    "# Session 생성, with 구문 안에서 유지\n",
    "with requests.Session() as s:\n",
    "    # 우선 클리앙 홈페이지에 들어가 봅시다.\n",
    "    first_page = s.get('https://www.clien.net/service')\n",
    "    html = first_page.text\n",
    "    soup = bs(html, 'html.parser')\n",
    "    csrf = soup.find('input', {'name': '_csrf'}) # input태그 중에서 name이 _csrf인 것을 찾습니다.\n",
    "    print(csrf['value']) # 위에서 찾은 태그의 value를 가져옵니다.\n",
    "\n",
    "    # 이제 LOGIN_INFO에 csrf값을 넣어줍시다.\n",
    "    # (p.s.)Python3에서 두 dict를 합치는 방법은 {**dict1, **dict2} 으로 dict들을 unpacking하는 것입니다.\n",
    "    LOGIN_INFO = {**LOGIN_INFO, **{'_csrf': csrf['value']}}\n",
    "    print(LOGIN_INFO)\n",
    "\n",
    "    # 이제 다시 로그인을 해봅시다.\n",
    "    login_req = s.post('https://www.clien.net/service/login', data=LOGIN_INFO)\n",
    "    # 어떤 결과가 나올까요? (200이면 성공!)\n",
    "    print(login_req.status_code)\n",
    "    # 로그인이 되지 않으면 경고를 띄워줍시다.\n",
    "    if login_req.status_code != 200:\n",
    "        raise Exception('로그인이 되지 않았어요! 아이디와 비밀번호를 다시한번 확인해 주세요.')\n",
    "\n",
    "    # -- 여기서부터는 로그인이 된 세션이 유지됩니다 --\n",
    "    # 이제 장터의 게시글 하나를 가져와 봅시다. 아래 예제 링크는 중고장터 공지글입니다.\n",
    "    post_one = s.get('https://www.clien.net/service/board/rule/10707408')\n",
    "    soup = bs(post_one.text, 'html.parser') # Soup으로 만들어 줍시다.\n",
    "    # 아래 CSS Selector는 공지글 제목을 콕 하고 집어줍니다.\n",
    "    title = soup.select('#div_content > div.post-title > div.title-subject > div')\n",
    "    contents = soup.select('#div_content > div.post.box > div.post-content > div.post-article.fr-view')\n",
    "    # HTML을 제대로 파싱한 뒤에는 .text속성을 이용합니다.\n",
    "    print(title[0].text) # 글제목의 문자만을 가져와봅시다.\n",
    "    # [0]을 하는 이유는 select로 하나만 가져와도 title자체는 리스트이기 때문입니다.\n",
    "    # 즉, 제목 글자는 title이라는 리스트의 0번(첫번째)에 들어가 있습니다.\n",
    "    print(contents[0].text) # 글내용도 마찬가지겠지요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0654e3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selenium은 webdriver라는 것을 통해 디바이스에 설치된 브라우저들을 제어할 수 있다.\n",
    "# Chrome WebDriver \n",
    "# PhantomJS webdriver: WebTesting을 위해 나온 Headless Browser다.(즉, 화면이 존재하지 않는다)\n",
    "\n",
    "from selenium import webdriver\n",
    "\n",
    "## Chrome의 경우 | 아까 받은 chromedriver의 위치를 지정해준다.\n",
    "driver = webdriver.Chrome('/Users/Jeongyeop/Crawler/chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "196c449b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "driver = webdriver.Chrome('/Users/Jeongyeop/Crawler/chromedriver.exe')\n",
    "#암묵적으로 웹 자원 로드를 위해 3초를 기다려 준다.\n",
    "driver.implicitly_wait(3)\n",
    "\n",
    "driver.get('https://google.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f490a915",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selenium은 driver객체를 통해 여러가지 메소드를 제공한다.\n",
    "# url에 접근하는 api, get('http://url.com')\n",
    "\n",
    "#페이지의 단일 element에 접근하는 api,\n",
    "# find_element_by_name('HTML_name')\n",
    "# find_element_by_id('HTML_id')\n",
    "# find_element_by_xpath('/html/body/some/xpath')\n",
    "\n",
    "## 페이지의 여러 elements에 접근하는 api등이 있다.\n",
    "# find_element_by_css_selector('#css>div.selector')\n",
    "# find_element_by_class_name('some_class_name')\n",
    "# find_element_by_tag_name('h1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "912f10d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네이버 로그인하기\n",
    "from selenium import webdriver\n",
    "\n",
    "driver = webdriver.Chrome('/Users/Jeongyeop/Crawler/chromedriver.exe')\n",
    "driver.implicitly_wait(3)\n",
    "## url에 접근한다.\n",
    "driver.get('https://nid.naver.com/nidlogin.login')\n",
    "\n",
    "## 아이디/비밀번호를 입력해준다.\n",
    "driver.find_element_by_name('id').send_keys('ID')\n",
    "driver.find_element_by_name('pw').send_keys('password')\n",
    "driver.find_element_by_xpath('//*[@id=\"frmNIDLogin\"]/fieldset/input').click()\n",
    "\n",
    "## 자동입력 방지문자로 이 방법으로 로그인 할 수 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e61b779",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
