{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e7d017f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영화리뷰 긍/부정 분류때 학습시켜 사용했던 모델을 이용해 blog 검색결과를 긍/부정으로 분류하고\n",
    "# 각 확률을 점수화해보기\n",
    "# naver_search_blog에서 저장한 파일을 이용해서 웹툰별 블로그 글 (최대 100)개에 대해 긍/부정 점수를 매긴다.\n",
    "# 1. positive와 negative로 분류해서 점수매긴 파일\n",
    "# 2. positive는 높은 점수, negative는 낮은 점수로 합산해 단일점수를 매긴 파일\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import urllib.request\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92ad9d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_table('ratings_train.txt')\n",
    "test_data = pd.read_table('ratings_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c394045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련용 리뷰 개수 : 150000\n"
     ]
    }
   ],
   "source": [
    "print('훈련용 리뷰 개수 :',len(train_data)) # 훈련용 리뷰 개수 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0af09d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop_duplicates(subset=['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "623298a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b20636e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'do you expect people to read the FAQ etc and actually accept hard atheism'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'do!!! you expect... people~ to~ read~ the FAQ, etc. and actually accept hard~! atheism?@@'\n",
    "re.sub(r'[^a-zA-Z ]', '', text) #알파벳과 공백을 제외하고 모두 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4f59a9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠포스터보고 초딩영화줄오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 솔직히 재미는 없다평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화스파이더맨에서 늙어보이기만 했던 커스틴 던...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                  아 더빙 진짜 짜증나네요 목소리      0\n",
       "1   3819312                         흠포스터보고 초딩영화줄오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                          교도소 이야기구먼 솔직히 재미는 없다평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화스파이더맨에서 늙어보이기만 했던 커스틴 던...      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\",regex=True)\n",
    "# 한글과 공백을 제외하고 모두 제거\n",
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef96f539",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['document'] = train_data['document'].str.replace('^ +', \"\", regex = True) # white space 데이터를 empty value로 변경\n",
    "train_data['document'].replace('', np.nan, inplace=True)\n",
    "#기존에 한글이 없던 리뷰는 빈값이 되었을 것이므로 다시 null값 찾아야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f81e832c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.dropna(how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "932e887e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.drop_duplicates(subset = ['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\n",
    "test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\",regex = True) # 정규 표현식 수행\n",
    "test_data['document'] = test_data['document'].str.replace('^ +', \"\", regex = True) # 공백은 empty 값으로 변경\n",
    "test_data['document'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n",
    "test_data = test_data.dropna(how='any') # Null 값 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f3d0c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#불용어 제거\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10bb58c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오다', '이렇다', '것', '도', '영화', '라고', '차라리', '뮤직비디오', '를', '만들다', '게', '나다', '뻔']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "okt = Okt()\n",
    "okt.morphs('와 이런 것도 영화라고 차라리 뮤직비디오를 만드는 게 나을 뻔', stem = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d50ef522",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "for sentence in train_data['document']:\n",
    "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "    X_train.append(temp_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2037088d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "for sentence in test_data['document']:\n",
    "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "    X_test.append(temp_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1eeaf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정수 인코딩\n",
    "# vocaburary 만들기\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e057a150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 43752\n",
      "등장 빈도가 2번 이하인 희귀 단어의 수: 24337\n",
      "단어 집합에서 희귀 단어의 비율: 55.62488571950996\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 1.8715872104872904\n"
     ]
    }
   ],
   "source": [
    "threshold = 3\n",
    "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3270d18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 19416\n"
     ]
    }
   ],
   "source": [
    "# 희귀단어의 비율은 전체의 절반 이상인것에 비해 총 희귀단어 등장 빈도는 1.8%이므로 자연어 처리에 중요해보이지 않음\n",
    "# 전체 단어 개수 중 빈도수 2이하인 단어는 제거.\n",
    "# 0번 패딩 토큰을 고려하여 + 1\n",
    "vocab_size = total_cnt - rare_cnt + 1\n",
    "print('단어 집합의 크기 :',vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c7cb76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(vocab_size) \n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fd879a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(train_data['label'])\n",
    "y_test = np.array(test_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e88aa7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e02ecad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jeongyeop\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "# 빈 샘플들을 제거\n",
    "X_train = np.delete(X_train, drop_train, axis=0)\n",
    "y_train = np.delete(y_train, drop_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4c0666e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 30\n",
    "X_train = pad_sequences(X_train, maxlen = max_len)\n",
    "X_test = pad_sequences(X_test, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "183a23e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e2b40cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1527/1527 [==============================] - 20s 13ms/step - loss: 0.3315 - acc: 0.8555\n",
      "\n",
      " 테스트 정확도: 0.8555\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_model('movieReview_best_model.h5')\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "415e8d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_predict(new_sentence):\n",
    "    new_sentence = okt.morphs(new_sentence, stem=True) # 토큰화\n",
    "    new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거\n",
    "    encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\n",
    "    pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩\n",
    "    score = float(loaded_model.predict(pad_new)) # 예측\n",
    "    if(score > 0.5):\n",
    "        #print(\\\"{:.2f}% 확률로 긍정 리뷰입니다.\\\\n\\\".format(score * 100))\n",
    "        return score*100\n",
    "    else:\n",
    "        #print(\\\"{:.2f}% 확률로 부정 리뷰입니다.\\\\n\\\".format((1 - score) * 100))\n",
    "        return (1-score)*-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "84f10d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# naver_search_blog에서 저장한 파일\n",
    "import json\n",
    "with open('data/NaverWebtoonData.json', encoding='utf-8') as jsonFile:\n",
    "    jsonData = json.load(jsonFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "43369602",
   "metadata": {},
   "outputs": [],
   "source": [
    "newList = []\n",
    "for i in range(len(jsonData)):\n",
    "    newDict = {}\n",
    "    cnt_p = 0\n",
    "    cnt_n = 0\n",
    "    total_p = 0\n",
    "    total_n = 0\n",
    "    #print(jsonData[i]['title'])\n",
    "    length = len(jsonData[i]['blog_description'])\n",
    "    #print(f\"총 {length}개의 데이터가 있습니다.\")\n",
    "    for str in jsonData[i]['blog_description']:\n",
    "        PN = sentiment_predict(str)\n",
    "        if PN > 0:\n",
    "            cnt_p += 1\n",
    "            total_p += PN\n",
    "        else :\n",
    "            cnt_n += 1\n",
    "            total_n -= PN\n",
    "    newDict['title'] = jsonData[i]['title']\n",
    "    newDict['nSearch'] = cnt_p + cnt_n\n",
    "    if cnt_p != 0 :\n",
    "        #print(f\"총 {cnt_p}개의 긍정리뷰가 있고 평균확률은 {total_p//cnt_p} 입니다.\")\n",
    "        newDict['positive review'] = int(total_p)\n",
    "    else :\n",
    "        newDict['positive review'] = 0\n",
    "    if cnt_n != 0 :\n",
    "        #print(f\"총 {cnt_n}개의 부정리뷰가 있고 평균확률은 {total_n//cnt_n} 입니다.\")\n",
    "        newDict['negative review'] = int(total_n)\n",
    "    else :\n",
    "        newDict['negative review'] = 0\n",
    "    \n",
    "    newList.append(newDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9f080da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#json 파일로 저장\n",
    "file_path = 'data/dataPN.json'\n",
    "with open(file_path, 'w', encoding = \"utf-8\") as outfile:\n",
    "    outfile.write(json.dumps(newList, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5fec2ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': '가비지타임',\n",
       "  'nSearch': 100,\n",
       "  'positive review': 5513,\n",
       "  'negative review': 2546},\n",
       " {'title': '겟백',\n",
       "  'nSearch': 100,\n",
       "  'positive review': 4825,\n",
       "  'negative review': 3093}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newList[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e0bbd270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_predict_score(new_sentence):\n",
    "    new_sentence = okt.morphs(new_sentence, stem=True) # 토큰화\n",
    "    new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거\n",
    "    encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\n",
    "    pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩\n",
    "    score = float(loaded_model.predict(pad_new)) # 예측\n",
    "    if(score > 0.5):\n",
    "        return score*100\n",
    "    else:\n",
    "        return score*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6910f45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "newList = []\n",
    "for i in range(len(jsonData)):\n",
    "    newDict = {}\n",
    "    cnt = 0\n",
    "    totalScore = 0\n",
    "    length = len(jsonData[i]['blog_description'])\n",
    "    for str in jsonData[i]['blog_description']:\n",
    "        score = sentiment_predict_score(str)\n",
    "        cnt += 1\n",
    "        totalScore += score\n",
    "\n",
    "    newDict['title'] = jsonData[i]['title']\n",
    "    if cnt != 0 :\n",
    "        newDict['nSearch'] = cnt\n",
    "        newDict['review_score'] = int(totalScore)\n",
    "    else :\n",
    "        newDict['nSearch'] = 0\n",
    "        newDict['review_score'] = 0\n",
    "\n",
    "    \n",
    "    newList.append(newDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1f8c4e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#json 파일로 저장\n",
    "file_path = 'data/dataReviewScore.json'\n",
    "with open(file_path, 'w', encoding = \"utf-8\") as outfile:\n",
    "    outfile.write(json.dumps(newList, ensure_ascii=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
